{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imperfect information games and RL\n",
    "During this project I\\`m going to investigtes difference between  perfect information and imperfect information games.\n",
    "\n",
    "The main problem of imperfect information games is that you can not forget about previous states like in chess. So in chess you are given postition on board and all privios moves does not metter, because give only this information you could do good move. On that other hand there is a poker which is imperfect information game. Giving that you are on river(final card). You have to know all previous moves of other users because it defi\n",
    "nes the probability of state that you are in. So to find best tactic in any game position you should solve all tree and is $10^{161}$ states to solve.\n",
    "    \n",
    "Optimal strategy for zero-sum game with two players is Nash equilibrium. Nash equilibrium is state in which each player playing his optimal strategy and deviation from this strategies will not make it better. So if you play nesh nash equilibrium strategy you will not lose(but you not abusing mistakes of other players). For finite game with two players ther is guaranteed to exist nash equilibrium.\n",
    "\n",
    "Current state-of-art poker bot \"Libratus\" is simplifying game, solving simple version and then extrapoleting this result on bigger game. This is done because to solve game you should solve all tree and this is not feasible. So right now they are doing somethis similar to DeepBlue. This algorims is running on supercomputer\n",
    "\n",
    "For more detail see\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=2dX0lwaQRX0\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/2dX0lwaQRX0/0.jpg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"480\" height=\"360\" border=\"10\" /></a>\n",
    "\n",
    "Is scope of this project I will work with [Kuhn poker](https://en.wikipedia.org/wiki/Kuhn_poker). This is simpified version of poker. In Kuhn poker, the deck includes only three playing cards, for example a King, Queen, and Jack. One card is dealt to each player, which may place bets similarly to a standard poker. If both players bet or both players pass, the player with the higher card wins, otherwise, the betting player wins. In scope if this work i will try to find Nash equilibrium for this game\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "from roomai.kuhn import *;\n",
    "import roomai\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "debug = False\n",
    "\n",
    "action_bet = 'bet'\n",
    "action_check = 'check'\n",
    "state_space = [(0, None, 0), (0, None, 1), (0, None, 2),  \n",
    "               (1, action_bet, 0), (1, action_bet, 1), (1, action_bet, 2), \n",
    "               (1, action_check, 0), (1, action_check, 1), (1, action_check, 2),\n",
    "               (2, action_bet, 0), (2, action_bet, 1), (2, action_bet, 2), \n",
    "               (2, action_check, 0), (2, action_check, 1), (2, action_check, 2)]\n",
    "\n",
    "\n",
    "n_states = len(state_space)\n",
    "action_space = [action_bet, action_check]\n",
    "n_actions = len(action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyActionPlayer(roomai.common.AbstractPlayer):\n",
    "    \"\"\"\n",
    "    Implementtion of Kuhn poker player that play only one card\n",
    "    \"\"\"\n",
    "    def receive_info(self, info):\n",
    "        self.state = get_state(info)\n",
    "        pass\n",
    "    \n",
    "    def __init__(self, action):\n",
    "        self.action = action\n",
    "        \n",
    "    def take_action(self):\n",
    "        if debug:\n",
    "            print \"dummy user state \", self.state\n",
    "        return roomai.kuhn.KuhnPokerAction.lookup(self.action)\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "class AgresivePlayer(roomai.common.AbstractPlayer):\n",
    "    \"\"\"\n",
    "    Kuhn poker player that always bet when he have good card\n",
    "    \"\"\"\n",
    "    def receive_info(self, info):\n",
    "        self.number = info.person_state.number \n",
    "        self.state = get_state(info)\n",
    "            \n",
    "    def take_action(self):\n",
    "        if debug:\n",
    "            print \"Agresive user state \", self.state\n",
    "        if self.number >= 1:\n",
    "            return roomai.kuhn.KuhnPokerAction.lookup(\"bet\")\n",
    "        else:\n",
    "            return roomai.kuhn.KuhnPokerAction.lookup(\"check\")\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figth_players(player1, player2, iterations=30000):\n",
    "    players = [player1] + [player2] + [roomai.common.RandomPlayerChance()]\n",
    "    env = KuhnPokerEnv()\n",
    "    total_scores = [0,0]\n",
    "    for i in range(iterations):\n",
    "        scores = KuhnPokerEnv.compete(env, players)\n",
    "        total_scores[0] += scores[0]\n",
    "        total_scores[1] += scores[1]\n",
    "    return [s/iterations for s in total_scores]\n",
    "\n",
    "def get_state(info):\n",
    "    prev_action = None\n",
    "    for action in info.public_state.action_history[::-1]:\n",
    "        if action[0] != 2:\n",
    "            prev_action = action[1].key\n",
    "            break\n",
    "            \n",
    "    return (info.public_state.epoch, prev_action, info.person_state.number)\n",
    "\n",
    "check_actor = DummyActionPlayer(\"check\")\n",
    "bet_actor =  DummyActionPlayer(\"bet\")\n",
    "agresive_actor = AgresivePlayer()\n",
    "random_actor = roomai.common.RandomPlayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rand_theta():\n",
    "\treturn np.array([random.uniform(0, 1) for j in range(n_states)])\n",
    "\n",
    "def action_from_prob(prob):\n",
    "    random.seed(datetime.now())\n",
    "    return np.random.choice([action_bet, action_check], p=[prob, 1-prob])\n",
    "\n",
    "class GeneticPlayer(roomai.common.AbstractPlayer):\n",
    "    \"\"\"\n",
    "    Player that uses genetic algorim ro find optimal strategy\n",
    "    \"\"\"\n",
    "    def receive_info(self, info):\n",
    "        self.number = info.person_state.number \n",
    "        self.state = get_state(info)\n",
    "        scores = info.public_state.scores\n",
    "        if scores:\n",
    "            reward = scores[info.person_state.id]\n",
    "            self.policy_rewards[self.current_policy] += reward\n",
    "            self.current_policy = (self.current_policy + 1) % self.children_count\n",
    "            self.iteration += 1\n",
    "            if self.iteration >= 400:\n",
    "                self.iteration = 0\n",
    "                self.update_policies()\n",
    "    \n",
    "    def update_policies(self):\n",
    "        #print self.policy_rewards\n",
    "        indexs = np.argsort(self.policy_rewards)[::-1]\n",
    "        #print indexs\n",
    "        best4 = [self.children_policies[indexs[i]] for i in range(4)]\n",
    "        muration4 = []\n",
    "        for _ in range(4):\n",
    "            perents = random.sample(best4, 2)\n",
    "            muration4.append([ (perents[0][i]+perents[1][i])/2 for i in range(n_states) ])\n",
    "        \n",
    "        self.children_policies = best4 + muration4 + [rand_theta() for _ in range(2)]\n",
    "        self.policy_rewards = [0 for _ in range(self.children_count)]\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.iteration = 0\n",
    "        self.children_count = 10\n",
    "        self.children_policies = [rand_theta() for _ in range(self.children_count)]\n",
    "        self.current_policy = 0\n",
    "        self.policy_rewards = [0 for _ in range(self.children_count)]\n",
    "            \n",
    "    def take_action(self):\n",
    "        state_index = state_space.index(self.state)\n",
    "        action_prob = self.children_policies[self.current_policy][state_index]\n",
    "        policy_action = action_from_prob(action_prob)\n",
    "        if debug:\n",
    "            print \"action!! state\", self.state\n",
    "            #print action_prob\n",
    "            #print policy_action\n",
    "        return roomai.kuhn.KuhnPokerAction.lookup(policy_action)\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPlayer(roomai.common.AbstractPlayer):\n",
    "    def receive_info(self, info):\n",
    "        self.state = get_state(info)\n",
    "    \n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        \n",
    "    def take_action(self):\n",
    "        state_index = state_space.index(self.state)\n",
    "        action_prob = self.policy[state_index]\n",
    "        policy_action = action_from_prob(action_prob)\n",
    "        if debug:\n",
    "            print \"PolicyPlayer state\", self.state\n",
    "            print action_prob\n",
    "            print policy_action\n",
    "        return roomai.kuhn.KuhnPokerAction.lookup(policy_action)\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "geneticPlayer = GeneticPlayer()\n",
    "debug = False\n",
    "for _ in range(10000):\n",
    "    figth_players(geneticPlayer, check_actor, 1)\n",
    "    figth_players(geneticPlayer, bet_actor, 1)\n",
    "    figth_players(geneticPlayer, agresive_actor, 1)\n",
    "    figth_players(geneticPlayer, random_actor, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal strategy\n",
    "Nash equilibrium mixed-strategy for Kuhn_poker taken from [wiki](https://en.wikipedia.org/wiki/Kuhn_poker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://en.wikipedia.org/wiki/Kuhn_poker\n",
    "nash_equilibrium = [0.33, 0,    1,  \n",
    "                    0,    0,    1, \n",
    "                    0.33, 0.33, 1,\n",
    "                    0.33, 0.66, 1, \n",
    "                    0.33, 0,    1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetic vs nash [-0.2212, 0.2212]\n"
     ]
    }
   ],
   "source": [
    "best_policy = geneticPlayer.children_policies[0]\n",
    "print 'genetic vs nash', figth_players(PolicyPlayer(best_policy), PolicyPlayer(nash_equilibrium), 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From wiki we know that the first player should expect to lose at a rate of ~ −1/18 per hand \n",
    "So we found close to near strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
